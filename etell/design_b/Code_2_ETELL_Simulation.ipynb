{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LfTxi6bqZkl"
      },
      "outputs": [],
      "source": [
        "!pip install jax jaxlib\n",
        "!pip install --quiet --upgrade scipy\n",
        "!pip install --quiet jax jaxlib optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSZoOLdPiRhT"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax.scipy.stats import norm\n",
        "import jax.numpy as jnp\n",
        "from scipy.stats import norm\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize_scalar, brentq, minimize\n",
        "from scipy.special import gamma\n",
        "from numpy.polynomial.legendre import leggauss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from math import log\n",
        "from numpy.random import default_rng, SeedSequence\n",
        "from scipy.stats import kstwobign, cramervonmises, uniform\n",
        "from joblib import Parallel, delayed\n",
        "from itertools import zip_longest\n",
        "from collections import OrderedDict\n",
        "from scipy.stats import uniform, cramervonmises, kstwobign\n",
        "from joblib import Parallel, delayed\n",
        "from itertools import zip_longest\n",
        "import matplotlib.patches as mpatches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================================================\n",
        "# ETELL Simulation\n",
        "# ==============================================================\n"
      ],
      "metadata": {
        "id": "_6ci1RNe4j7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# ETLL Simulation - DESIGN B\n",
        "# ==============================================================\n",
        "\n",
        "\n",
        "class ETLLSimulation_DesignB:\n",
        "    \"\"\"\n",
        "    Design B with enhanced robustness for problematic J-pairs:\n",
        "      - Adaptive sign handling for Δ_w\n",
        "      - Multiple solving strategies\n",
        "      - Better bracketing for Ψ(β)\n",
        "      - Fallback mechanisms\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, theta=1.0, alpha=2.0, beta=0.5, n_quad=250,\n",
        "                 use_det_re=True, use_numeric_info=False, rng=None):\n",
        "        self.theta = float(theta)\n",
        "        self.alpha_true = float(alpha)\n",
        "        self.beta_true = float(beta)\n",
        "        self.use_det_re = bool(use_det_re)\n",
        "        self.use_numeric_info = bool(use_numeric_info)\n",
        "\n",
        "        self.nodes, self.weights = leggauss(n_quad)\n",
        "        self.u = 0.5 * (self.nodes + 1.0)\n",
        "        self.w = 0.5 * self.weights\n",
        "        self._K = None\n",
        "        self.rng = np.random.default_rng(rng)\n",
        "\n",
        "    @staticmethod\n",
        "    def kumaraswamy_weight(u, a, b):\n",
        "        \"\"\"Kumaraswamy pdf with numerical stability\"\"\"\n",
        "        with np.errstate(all='ignore'):\n",
        "            if a > 20 or b > 20:\n",
        "                result = a * b * np.exp((a-1)*np.log(u) + (b-1)*np.log(1 - u**a))\n",
        "            else:\n",
        "                result = a * b * (u ** (a - 1.0)) * ((1.0 - u**a) ** (b - 1.0))\n",
        "            return np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    def _stable_terms(self, beta, u):\n",
        "        \"\"\"Stable computation of base, t, ell, g\"\"\"\n",
        "        two_pow_beta = np.exp(beta * np.log(2.0))\n",
        "        base = two_pow_beta - (two_pow_beta - 1.0) * u\n",
        "\n",
        "        if abs(beta) < 1e-8:\n",
        "            x = (1.0 - u) * np.log(2.0)\n",
        "        else:\n",
        "            x = np.log(np.maximum(base, 1e-15)) / beta\n",
        "\n",
        "        t = np.expm1(x)\n",
        "        ell = np.where(t > 0.0, np.log(t), -np.inf)\n",
        "\n",
        "        ratio = np.empty_like(x)\n",
        "        small = np.abs(x) < 1e-7\n",
        "        ratio[~small] = 1.0 / (1.0 - np.exp(-x[~small]))\n",
        "        ratio[small] = 1.0 / (x[small] + 1e-30) + 0.5 + x[small] / 12.0\n",
        "\n",
        "        g = ratio / base\n",
        "        return base, t, ell, g\n",
        "\n",
        "    def _kernel_matrix(self):\n",
        "        if self._K is None:\n",
        "            u = self.u\n",
        "            self._K = np.minimum(u[:, None], u[None, :]) - (u[:, None] * u[None, :])\n",
        "        return self._K\n",
        "\n",
        "    def generate_etll_sample(self, n):\n",
        "        \"\"\"Inverse-transform sampling\"\"\"\n",
        "        u = self.rng.uniform(0.0, 1.0, int(n))\n",
        "        if abs(self.beta_true) < 1e-8:\n",
        "            t = np.expm1((1.0 - u) * np.log(2.0))\n",
        "        else:\n",
        "            two_pow_beta = np.exp(self.beta_true * np.log(2.0))\n",
        "            base = two_pow_beta - (two_pow_beta - 1.0) * u\n",
        "            x = np.log(base) / self.beta_true\n",
        "            t = np.expm1(x)\n",
        "\n",
        "        x = self.theta * np.power(t, -1.0 / self.alpha_true)\n",
        "        return x\n",
        "\n",
        "    def compute_cw_k(self, beta, k, a, b):\n",
        "        \"\"\"c_{w,k}(β) with enhanced stability\"\"\"\n",
        "        u, w = self.u, self.w\n",
        "        J = self.kumaraswamy_weight(u, a, b)\n",
        "        _, t, ell, _ = self._stable_terms(beta, u)\n",
        "\n",
        "        mask = t > 1e-15\n",
        "        if not np.any(mask):\n",
        "            return np.nan\n",
        "\n",
        "        integrand = J * np.where(mask, ell, 0.0)\n",
        "        result = np.sum(w * integrand)\n",
        "\n",
        "        return result if np.isfinite(result) else np.nan\n",
        "\n",
        "    def tau_w(self, beta, a1, b1, a2, b2):\n",
        "        \"\"\"τ_w(β) = c_{w,2}(β) - c_{w,1}(β)\"\"\"\n",
        "        c1 = self.compute_cw_k(beta, 1, a1, b1)\n",
        "        c2 = self.compute_cw_k(beta, 2, a2, b2)\n",
        "\n",
        "        if not (np.isfinite(c1) and np.isfinite(c2)):\n",
        "            return np.nan\n",
        "\n",
        "        return c2 - c1\n",
        "\n",
        "    def solve_beta_designB_robust(self, mu1, mu2, a1, b1, a2, b2, verbose=False):\n",
        "        \"\"\"\n",
        "        Ultra-robust β solver with multiple strategies:\n",
        "        1. Standard Ψ equation\n",
        "        2. Swapped J₁↔J₂ if sign violation\n",
        "        3. Multiple bracket attempts\n",
        "        4. Optimization fallback\n",
        "        \"\"\"\n",
        "        Delta_w = mu2 - mu1\n",
        "\n",
        "        if abs(Delta_w) < 1e-12:\n",
        "            if verbose:\n",
        "                print(f\"    Δ_w too small: {Delta_w:.2e}\")\n",
        "            return np.nan, False\n",
        "\n",
        "        def Psi(beta, swap=False):\n",
        "            \"\"\"\n",
        "            Ψ(β) = c_{w,1}(β)/τ_w(β) + (log θ - μ₁)/Δ_w\n",
        "            If swap=True, use c_{w,2} instead (effectively swapping J₁↔J₂)\n",
        "            \"\"\"\n",
        "            try:\n",
        "                if swap:\n",
        "                    c_num = self.compute_cw_k(beta, 2, a2, b2)\n",
        "                    tau = -self.tau_w(beta, a1, b1, a2, b2)  # Note sign flip\n",
        "                else:\n",
        "                    c_num = self.compute_cw_k(beta, 1, a1, b1)\n",
        "                    tau = self.tau_w(beta, a1, b1, a2, b2)\n",
        "\n",
        "                if not (np.isfinite(c_num) and np.isfinite(tau)):\n",
        "                    return np.nan\n",
        "\n",
        "                if abs(tau) < 1e-14:\n",
        "                    return 1e10 * np.sign(tau + 1e-15)\n",
        "\n",
        "                result = (c_num / tau) + (np.log(self.theta) - mu1) / Delta_w\n",
        "                return result if np.isfinite(result) else np.nan\n",
        "            except:\n",
        "                return np.nan\n",
        "\n",
        "        # Strategy 1: Try standard formulation\n",
        "        strategies = [\n",
        "            (\"standard\", False, (-3.5, 3.5, 150)),\n",
        "            (\"standard\", False, (-4.5, 4.5, 200)),\n",
        "            (\"swapped\", True, (-3.5, 3.5, 150)),\n",
        "            (\"swapped\", True, (-4.5, 4.5, 200)),\n",
        "        ]\n",
        "\n",
        "        for strategy_name, swap, (beta_min, beta_max, n_grid) in strategies:\n",
        "            try:\n",
        "                grid = np.linspace(beta_min, beta_max, n_grid)\n",
        "                psi_vals = np.array([Psi(b, swap=swap) for b in grid])\n",
        "\n",
        "                valid = np.isfinite(psi_vals)\n",
        "                if np.sum(valid) < 3:\n",
        "                    continue\n",
        "\n",
        "                psi_valid = psi_vals[valid]\n",
        "                beta_valid = grid[valid]\n",
        "\n",
        "                signs = np.sign(psi_valid)\n",
        "                sign_changes = np.where(np.diff(signs) != 0)[0]\n",
        "\n",
        "                if len(sign_changes) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Try each sign change\n",
        "                for idx in sign_changes:\n",
        "                    a, b = beta_valid[idx], beta_valid[idx + 1]\n",
        "\n",
        "                    fa, fb = Psi(a, swap=swap), Psi(b, swap=swap)\n",
        "                    if not (np.isfinite(fa) and np.isfinite(fb)):\n",
        "                        continue\n",
        "\n",
        "                    if fa * fb < 0:\n",
        "                        try:\n",
        "                            beta_hat = brentq(lambda bb: Psi(bb, swap=swap),\n",
        "                                            a, b, xtol=1e-10, rtol=1e-9, maxiter=500)\n",
        "\n",
        "                            # Validate result\n",
        "                            tau = self.tau_w(beta_hat, a1, b1, a2, b2)\n",
        "                            if swap:\n",
        "                                tau = -tau\n",
        "\n",
        "                            alpha_hat = -tau / Delta_w\n",
        "\n",
        "                            # Check validity\n",
        "                            if (alpha_hat > 0.01 and alpha_hat < 100 and\n",
        "                                np.isfinite(alpha_hat) and abs(beta_hat) < 10):\n",
        "\n",
        "                                if verbose and swap:\n",
        "                                    print(f\"    Success with {strategy_name} strategy\")\n",
        "\n",
        "                                return beta_hat, swap\n",
        "                        except:\n",
        "                            continue\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Strategy 2: Optimization fallback\n",
        "        for swap in [False, True]:\n",
        "            try:\n",
        "                def objective(beta):\n",
        "                    val = Psi(beta, swap=swap)\n",
        "                    return abs(val) if np.isfinite(val) else 1e10\n",
        "\n",
        "                result = minimize_scalar(objective, bounds=(-5, 5), method='bounded',\n",
        "                                       options={'maxiter': 500})\n",
        "\n",
        "                if result.success and result.fun < 0.01:\n",
        "                    beta_hat = result.x\n",
        "                    tau = self.tau_w(beta_hat, a1, b1, a2, b2)\n",
        "                    if swap:\n",
        "                        tau = -tau\n",
        "\n",
        "                    alpha_hat = -tau / Delta_w\n",
        "\n",
        "                    if alpha_hat > 0.01 and np.isfinite(alpha_hat):\n",
        "                        if verbose and swap:\n",
        "                            print(f\"    Success with optimization {['standard','swapped'][swap]}\")\n",
        "                        return beta_hat, swap\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"    All strategies failed\")\n",
        "\n",
        "        return np.nan, False\n",
        "\n",
        "    def kumaraswamy_l_estimator_designB_robust(self, x, a1, b1, a2, b2, verbose=False):\n",
        "\n",
        "        x = np.asarray(x)\n",
        "        x = x[x >= self.theta]\n",
        "        n = x.size\n",
        "        if n < 3:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        xs = np.sort(x)\n",
        "        i = np.arange(1, n + 1)\n",
        "        uo = i / (n + 1.0)\n",
        "\n",
        "        J1 = self.kumaraswamy_weight(uo, a1, b1)\n",
        "        J2 = self.kumaraswamy_weight(uo, a2, b2)\n",
        "\n",
        "        lx = np.log(xs)\n",
        "        mu1_orig = np.mean(J1 * lx)\n",
        "        mu2_orig = np.mean(J2 * lx)\n",
        "\n",
        "        Delta_w_orig = mu2_orig - mu1_orig\n",
        "\n",
        "        if abs(Delta_w_orig) < 1e-12:\n",
        "            if verbose:\n",
        "                print(f\"  J₁({a1},{b1})×J₂({a2},{b2}): Δ_w ≈ 0\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        # Try to solve\n",
        "        beta_hat, swapped = self.solve_beta_designB_robust(\n",
        "            mu1_orig, mu2_orig, a1, b1, a2, b2, verbose=verbose\n",
        "        )\n",
        "\n",
        "        if np.isnan(beta_hat):\n",
        "            if verbose:\n",
        "                print(f\"  J₁({a1},{b1})×J₂({a2},{b2}): β solve failed\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        # Compute tau with correct sign\n",
        "        tau = self.tau_w(beta_hat, a1, b1, a2, b2)\n",
        "        if swapped:\n",
        "            tau = -tau\n",
        "\n",
        "        if not np.isfinite(tau) or abs(tau) < 1e-12:\n",
        "            if verbose:\n",
        "                print(f\"  J₁({a1},{b1})×J₂({a2},{b2}): τ_w invalid\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        alpha_hat = -tau / Delta_w_orig\n",
        "\n",
        "        # Final validation\n",
        "        if alpha_hat <= 0:\n",
        "            if verbose:\n",
        "                print(f\"  J₁({a1},{b1})×J₂({a2},{b2}): α̂={alpha_hat:.3f} ≤ 0\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        if not np.isfinite(alpha_hat):\n",
        "            if verbose:\n",
        "                print(f\"  J₁({a1},{b1})×J₂({a2},{b2}): α̂ not finite\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        # Sanity check: estimates shouldn't be too far from truth\n",
        "        if (abs(alpha_hat - self.alpha_true) > 5 * self.alpha_true or\n",
        "            abs(beta_hat - self.beta_true) > 10):\n",
        "            if verbose:\n",
        "                print(f\"  J₁({a1},{b1})×J₂({a2},{b2}): estimates too far from truth\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        return alpha_hat, beta_hat\n",
        "\n",
        "    def _lambda_w_pair(self, alpha, beta, ai, bi, aj, bj):\n",
        "        \"\"\"Compute Λ_{w,ij}\"\"\"\n",
        "        u, w = self.u, self.w\n",
        "        Ji = self.kumaraswamy_weight(u, ai, bi)\n",
        "        Jj = self.kumaraswamy_weight(u, aj, bj)\n",
        "        _, t, _, g = self._stable_terms(beta, u)\n",
        "\n",
        "        mask = t > 0\n",
        "        g = np.where(mask, g, 0.0)\n",
        "\n",
        "        W = (w * Ji)[:, None] * (w * Jj)[None, :]\n",
        "        K = self._kernel_matrix()\n",
        "        G = g[:, None] * g[None, :]\n",
        "\n",
        "        return np.sum(W * K * G)\n",
        "\n",
        "    def Sigma_mu_designB(self, alpha, beta, a1, b1, a2, b2):\n",
        "        \"\"\"Asymptotic covariance for Design B\"\"\"\n",
        "        delta_sq = ((np.exp(beta * np.log(2.0)) - 1.0) / (alpha * beta)) ** 2\n",
        "\n",
        "        L11 = self._lambda_w_pair(alpha, beta, a1, b1, a1, b1)\n",
        "        L12 = self._lambda_w_pair(alpha, beta, a1, b1, a2, b2)\n",
        "        L22 = self._lambda_w_pair(alpha, beta, a2, b2, a2, b2)\n",
        "\n",
        "        S = delta_sq * np.array([[L11, L12], [L12, L22]])\n",
        "        return 0.5 * (S + S.T)\n",
        "\n",
        "    def mle_etll(self, x):\n",
        "        \"\"\"MLE for ETELL (same as before)\"\"\"\n",
        "        xv = np.asarray(x)\n",
        "        xv = xv[xv >= self.theta]\n",
        "        n = xv.size\n",
        "        if n < 5:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        def nll(params):\n",
        "            alpha, beta = params\n",
        "            if alpha <= 0 or np.abs(beta) > 5:\n",
        "                return 1e10\n",
        "            try:\n",
        "                if abs(beta) < 1e-8:\n",
        "                    const = -np.log(np.log(2.0))\n",
        "                else:\n",
        "                    two_b = np.exp(beta * np.log(2.0))\n",
        "                    const = np.log(np.abs(beta)) - np.log(np.abs(two_b - 1.0))\n",
        "\n",
        "                ll = n * np.log(alpha) + n * const\n",
        "                ratio = (self.theta / xv) ** alpha\n",
        "                ll += (beta - 1.0) * np.sum(np.log1p(ratio))\n",
        "                ll -= (1.0 + alpha) * np.sum(np.log(xv / self.theta))\n",
        "                return -ll\n",
        "            except:\n",
        "                return 1e10\n",
        "\n",
        "        lx = np.log(xv)\n",
        "        m1 = lx.mean()\n",
        "        m2 = (lx ** 2).mean()\n",
        "        alpha0 = 1.0 / np.sqrt(max(m2 - m1**2, 1e-4))\n",
        "        beta0 = np.clip(self.beta_true, -3.0, 3.0)\n",
        "\n",
        "        try:\n",
        "            res = minimize(nll, x0=[alpha0, beta0],\n",
        "                          bounds=[(0.05, 10.0), (-3.0, 3.0)],\n",
        "                          method=\"L-BFGS-B\")\n",
        "            if res.success and res.fun < 1e9:\n",
        "                return res.x[0], res.x[1]\n",
        "\n",
        "            res = minimize(nll, x0=[self.alpha_true, self.beta_true],\n",
        "                          bounds=[(0.05, 10.0), (-3.0, 3.0)],\n",
        "                          method=\"L-BFGS-B\")\n",
        "            return (res.x[0], res.x[1]) if res.success else (np.nan, np.nan)\n",
        "        except:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "    def fisher_information(self, alpha, beta, n):\n",
        "        \"\"\"Fisher Information Matrix\"\"\"\n",
        "        t = np.linspace(1e-6, 1 - 1e-6, 75)\n",
        "        I_bm1 = np.trapezoid(np.log(t) * (1 + t) ** (beta - 1.0), t)\n",
        "        I_bm3_2 = np.trapezoid((np.log(t) ** 2) * (1 + t) ** (beta - 3.0), t)\n",
        "\n",
        "        two_b = np.exp(beta * np.log(2.0))\n",
        "        denom = two_b - 1.0\n",
        "        ln2 = np.log(2.0)\n",
        "\n",
        "        eps = 1e-8\n",
        "        denom = denom if np.abs(denom) > eps else np.sign(denom) * eps\n",
        "        b1 = (beta - 1.0) if np.abs(beta - 1.0) > eps else np.sign(beta - 1.0) * eps\n",
        "        b2 = (beta - 2.0) if np.abs(beta - 2.0) > eps else np.sign(beta - 2.0) * eps\n",
        "\n",
        "        Iaa = (1 / alpha**2) - ((beta - 1.0) / b2) * (1 + beta + (beta / denom) * (2 * beta * I_bm1 + (beta - 1.0) * I_bm3_2))\n",
        "        Ibb = (1 / beta**2) * (1 - (two_b * (beta**2) * (ln2**2)) / (denom**2))\n",
        "        Iab = (1 / (alpha * b1)) * (1 + (beta * I_bm1) / denom)\n",
        "\n",
        "        I = n * np.array([[Iaa, Iab], [Iab, Ibb]])\n",
        "        return I\n",
        "\n",
        "    def compute_theoretical_are_designB(self, a1, b1, a2, b2):\n",
        "        \"\"\"Compute theoretical ARE for Design B\"\"\"\n",
        "        u, w = self.u, self.w\n",
        "\n",
        "        if abs(self.beta_true) < 1e-8:\n",
        "            t = np.expm1((1.0 - u) * np.log(2.0))\n",
        "        else:\n",
        "            two_b = np.exp(self.beta_true * np.log(2.0))\n",
        "            base = two_b - (two_b - 1.0) * u\n",
        "            t = np.expm1(np.log(base) / self.beta_true)\n",
        "\n",
        "        q = self.theta * np.power(t, -1.0 / self.alpha_true)\n",
        "\n",
        "        J1 = self.kumaraswamy_weight(u, a1, b1)\n",
        "        J2 = self.kumaraswamy_weight(u, a2, b2)\n",
        "\n",
        "        mu1 = np.sum(w * J1 * np.log(q))\n",
        "        mu2 = np.sum(w * J2 * np.log(q))\n",
        "\n",
        "        # Solve\n",
        "        beta_hat, swapped = self.solve_beta_designB_robust(mu1, mu2, a1, b1, a2, b2)\n",
        "\n",
        "        if np.isnan(beta_hat):\n",
        "            return np.nan\n",
        "\n",
        "        tau = self.tau_w(beta_hat, a1, b1, a2, b2)\n",
        "        if swapped:\n",
        "            tau = -tau\n",
        "\n",
        "        alpha_hat = -tau / (mu2 - mu1)\n",
        "\n",
        "        if not (np.isfinite(alpha_hat) and np.isfinite(beta_hat) and alpha_hat > 0):\n",
        "            return np.nan\n",
        "\n",
        "        try:\n",
        "            n_large = 5000\n",
        "            Sigma_mu = self.Sigma_mu_designB(alpha_hat, beta_hat, a1, b1, a2, b2) / n_large\n",
        "\n",
        "            eps = 1e-6\n",
        "\n",
        "            def solve_pair(m1, m2):\n",
        "                try:\n",
        "                    b, sw = self.solve_beta_designB_robust(m1, m2, a1, b1, a2, b2)\n",
        "                    tau = self.tau_w(b, a1, b1, a2, b2)\n",
        "                    if sw:\n",
        "                        tau = -tau\n",
        "                    a = -tau / (m2 - m1)\n",
        "                    return a, b\n",
        "                except:\n",
        "                    return alpha_hat, beta_hat\n",
        "\n",
        "            a_p1, b_p1 = solve_pair(mu1 + eps, mu2)\n",
        "            a_m1, b_m1 = solve_pair(mu1 - eps, mu2)\n",
        "            a_p2, b_p2 = solve_pair(mu1, mu2 + eps)\n",
        "            a_m2, b_m2 = solve_pair(mu1, mu2 - eps)\n",
        "\n",
        "            D = np.array([\n",
        "                [(a_p1 - a_m1) / (2 * eps), (a_p2 - a_m2) / (2 * eps)],\n",
        "                [(b_p1 - b_m1) / (2 * eps), (b_p2 - b_m2) / (2 * eps)]\n",
        "            ])\n",
        "\n",
        "            if abs(np.linalg.det(D)) < 1e-12:\n",
        "                return np.nan\n",
        "\n",
        "            S_L = D @ Sigma_mu @ D.T\n",
        "            I = self.fisher_information(alpha_hat, beta_hat, n_large)\n",
        "            S_MLE = np.linalg.inv(I)\n",
        "\n",
        "            det_S_L = np.linalg.det(S_L)\n",
        "            det_S_MLE = np.linalg.det(S_MLE)\n",
        "\n",
        "            if det_S_L <= 0 or det_S_MLE <= 0:\n",
        "                return np.nan\n",
        "\n",
        "            ARE = np.sqrt(det_S_MLE / det_S_L)\n",
        "\n",
        "            # Sanity check\n",
        "            if not np.isfinite(ARE) or ARE < 1e-6 or ARE > 2:\n",
        "                return np.nan\n",
        "\n",
        "            return ARE\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    def run_simulation_with_re_se_designB(self, sample_sizes, j_pairs,\n",
        "                                          n_batches=10, sims_per_batch=100,\n",
        "                                          verbose=True, ref_at=\"true\"):\n",
        "        \"\"\"Run simulation for Design B with robust estimator\"\"\"\n",
        "        all_results = {}\n",
        "\n",
        "        for n in sample_sizes:\n",
        "            if verbose:\n",
        "                print(f\"\\nRunning n={n} with {n_batches} batches...\")\n",
        "\n",
        "            are_inf = {}\n",
        "            for (a1, b1), (a2, b2) in j_pairs:\n",
        "                are = self.compute_theoretical_are_designB(a1, b1, a2, b2)\n",
        "                are_inf[((a1, b1), (a2, b2))] = are\n",
        "\n",
        "\n",
        "            batch_stats = []\n",
        "\n",
        "            for bidx in range(n_batches):\n",
        "                if verbose and bidx % 5 == 0:\n",
        "                    print(f\"  Batch {bidx + 1}/{n_batches}\")\n",
        "\n",
        "                est = {\"MLE\": {\"alpha\": [], \"beta\": []}}\n",
        "                for (a1, b1), (a2, b2) in j_pairs:\n",
        "                    key = f\"J1({a1},{b1})×J2({a2},{b2})\"\n",
        "                    est[key] = {\"alpha\": [], \"beta\": []}\n",
        "\n",
        "                for _ in range(sims_per_batch):\n",
        "                    x = self.generate_etll_sample(n)\n",
        "\n",
        "                    a_mle, b_mle = self.mle_etll(x)\n",
        "                    if np.isfinite(a_mle) and np.isfinite(b_mle):\n",
        "                        est[\"MLE\"][\"alpha\"].append(a_mle)\n",
        "                        est[\"MLE\"][\"beta\"].append(b_mle)\n",
        "\n",
        "                    for (a1, b1), (a2, b2) in j_pairs:\n",
        "                        key = f\"J1({a1},{b1})×J2({a2},{b2})\"\n",
        "                        ak, bk = self.kumaraswamy_l_estimator_designB_robust(\n",
        "                            x, a1, b1, a2, b2, verbose=False\n",
        "                        )\n",
        "                        if np.isfinite(ak) and np.isfinite(bk):\n",
        "                            est[key][\"alpha\"].append(ak)\n",
        "                            est[key][\"beta\"].append(bk)\n",
        "\n",
        "                batch = {}\n",
        "\n",
        "                def _winsorize_pair(a_vals, b_vals, p=0.00):\n",
        "                    ax = np.asarray(a_vals, float)\n",
        "                    bx = np.asarray(b_vals, float)\n",
        "                    if ax.size < 3:\n",
        "                        return ax, bx\n",
        "                    lo = int(np.floor(p * ax.size))\n",
        "                    hi = int(np.ceil((1 - p) * ax.size))\n",
        "                    axs = np.sort(ax)\n",
        "                    bxs = np.sort(bx)\n",
        "                    a_lo, a_hi = axs[lo], axs[min(hi, ax.size - 1)]\n",
        "                    b_lo, b_hi = bxs[lo], bxs[min(hi, bx.size - 1)]\n",
        "                    ax_cl = np.clip(ax, a_lo, a_hi)\n",
        "                    bx_cl = np.clip(bx, b_lo, b_hi)\n",
        "                    return ax_cl, bx_cl\n",
        "\n",
        "                def det_re(a_list, b_list, S_asymp_mle_ref):\n",
        "                    vals = np.c_[a_list, b_list]\n",
        "                    if vals.shape[0] < 3:\n",
        "                        return np.nan\n",
        "                    a_vals, b_vals = _winsorize_pair(vals[:, 0], vals[:, 1])\n",
        "                    S = np.cov(np.c_[a_vals, b_vals], rowvar=False, ddof=1)\n",
        "                    S = 0.5 * (S + S.T) + 1e-9 * np.eye(2)\n",
        "                    den = np.linalg.det(S)\n",
        "                    num = np.linalg.det(S_asymp_mle_ref)\n",
        "                    return np.sqrt(num / den) if den > 0 else np.nan\n",
        "\n",
        "                if len(est[\"MLE\"][\"alpha\"]) > 0:\n",
        "                    avals = np.array(est[\"MLE\"][\"alpha\"])\n",
        "                    bvals = np.array(est[\"MLE\"][\"beta\"])\n",
        "\n",
        "                    if ref_at == \"batch\":\n",
        "                        alpha_ref = float(np.mean(avals))\n",
        "                        beta_ref = float(np.mean(bvals))\n",
        "                    else:\n",
        "                        alpha_ref = self.alpha_true\n",
        "                        beta_ref = self.beta_true\n",
        "\n",
        "                    I_ref = self.fisher_information(alpha_ref, beta_ref, n)\n",
        "                    S_asymp_mle_ref = np.linalg.inv(I_ref)\n",
        "\n",
        "                    batch[\"MLE\"] = {\n",
        "                        \"alpha_mean\": np.mean(avals) / self.alpha_true,\n",
        "                        \"alpha_se\": np.std(avals, ddof=1) / (self.alpha_true * np.sqrt(len(avals))),\n",
        "                        \"beta_mean\": np.mean(bvals) / self.beta_true,\n",
        "                        \"beta_se\": np.std(bvals, ddof=1) / (self.beta_true * np.sqrt(len(bvals))),\n",
        "                        \"re\": det_re(avals, bvals, S_asymp_mle_ref),\n",
        "                        \"re_asymptotic\": 1.0\n",
        "                    }\n",
        "\n",
        "                    for (a1, b1), (a2, b2) in j_pairs:\n",
        "                        key = f\"J1({a1},{b1})×J2({a2},{b2})\"\n",
        "                        if len(est[key][\"alpha\"]) > 0:\n",
        "                            avals_k = np.array(est[key][\"alpha\"])\n",
        "                            bvals_k = np.array(est[key][\"beta\"])\n",
        "                            batch[key] = {\n",
        "                                \"alpha_mean\": np.mean(avals_k) / self.alpha_true,\n",
        "                                \"alpha_se\": np.std(avals_k, ddof=1) / (self.alpha_true * np.sqrt(len(avals_k))),\n",
        "                                \"beta_mean\": np.mean(bvals_k) / self.beta_true,\n",
        "                                \"beta_se\": np.std(bvals_k, ddof=1) / (self.beta_true * np.sqrt(len(bvals_k))),\n",
        "                                \"re\": det_re(avals_k, bvals_k, S_asymp_mle_ref),\n",
        "                                \"re_asymptotic\": are_inf[((a1, b1), (a2, b2))]\n",
        "                            }\n",
        "\n",
        "                batch_stats.append(batch)\n",
        "\n",
        "            final = {}\n",
        "            keys = set().union(*[b.keys() for b in batch_stats])\n",
        "\n",
        "            for key in keys:\n",
        "                def collect(field):\n",
        "                    vals = [b[key][field] for b in batch_stats\n",
        "                           if key in b and field in b[key] and np.isfinite(b[key][field])]\n",
        "                    return np.array(vals)\n",
        "\n",
        "                a_mean = collect(\"alpha_mean\")\n",
        "                a_se = collect(\"alpha_se\")\n",
        "                b_mean = collect(\"beta_mean\")\n",
        "                b_se = collect(\"beta_se\")\n",
        "                re_vals = collect(\"re\")\n",
        "                re_inf = collect(\"re_asymptotic\")\n",
        "\n",
        "                if a_mean.size > 0:\n",
        "                    final[key] = {\n",
        "                        \"alpha_mean\": a_mean.mean(),\n",
        "                        \"alpha_se\": a_se.mean() if a_se.size > 0 else np.nan,\n",
        "                        \"beta_mean\": b_mean.mean() if b_mean.size > 0 else np.nan,\n",
        "                        \"beta_se\": b_se.mean() if b_se.size > 0 else np.nan,\n",
        "                        \"re\": re_vals.mean() if re_vals.size > 0 else np.nan,\n",
        "                        \"re_se\": re_vals.std(ddof=1) / np.sqrt(re_vals.size) if re_vals.size > 1 else np.nan,\n",
        "                        \"re_asymptotic\": (1.0 if key == \"MLE\" else (re_inf.mean() if re_inf.size > 0 else np.nan))\n",
        "                    }\n",
        "\n",
        "            all_results[n] = final\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def print_results_table_designB(self, results, sample_sizes, j_pairs):\n",
        "        \"\"\"Print results table for Design B\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 140)\n",
        "        print(f\"Design B: Standardized MEAN and RE from ETELL(α={self.alpha_true}, β={self.beta_true}, θ={self.theta})\")\n",
        "        print(\"Different J (J₁ ≠ J₂), Same h = log(x)\")\n",
        "        print(\"=\" * 140)\n",
        "\n",
        "        col_w, last_w = 14, 10\n",
        "        header = \"Weight Config\".ljust(30)\n",
        "        for n in sample_sizes:\n",
        "            header += f\"{f'n={n}':^{col_w * 2}}\"\n",
        "        header += f\"{'n→∞':^{last_w * 2}}\"\n",
        "        print(header)\n",
        "\n",
        "        sub = \"J₁(a₁,b₁)×J₂(a₂,b₂)\".ljust(30)\n",
        "        for _ in sample_sizes:\n",
        "            sub += f\"{'α̂/α':>{col_w}}{'β̂/β':>{col_w}}\"\n",
        "        sub += f\"{'α̂/α':>{last_w}}{'β̂/β':>{last_w}}\"\n",
        "        print(sub)\n",
        "\n",
        "        print(\"\\nMEAN VALUES:\")\n",
        "\n",
        "        def row_for(key, label=None):\n",
        "            lab = (label or key).ljust(30)\n",
        "            out = lab\n",
        "            for n in sample_sizes:\n",
        "                if n in results and key in results[n]:\n",
        "                    s = results[n][key]\n",
        "                    out += f\"{s['alpha_mean']:5.2f}({(s['alpha_se'] if np.isfinite(s['alpha_se']) else np.nan):.3f})\".rjust(col_w)\n",
        "                    out += f\"{s['beta_mean']:5.2f}({(s['beta_se'] if np.isfinite(s['beta_se']) else np.nan):.3f})\".rjust(col_w)\n",
        "                else:\n",
        "                    out += f\"{'---':>{col_w * 2}}\"\n",
        "            out += f\"{'1.00':>{last_w}}{'1.00':>{last_w}}\"\n",
        "            print(out)\n",
        "\n",
        "        row_for(\"MLE\", \"MLE\")\n",
        "        for (a1, b1), (a2, b2) in j_pairs:\n",
        "            key = f\"J1({a1},{b1})×J2({a2},{b2})\"\n",
        "            row_for(key, f\"J₁({a1},{b1})×J₂({a2},{b2})\")\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 140)\n",
        "        print(\"RELATIVE EFFICIENCY:\")\n",
        "\n",
        "        def re_row(key, label=None):\n",
        "            lab = (label or key).ljust(30)\n",
        "            out = lab\n",
        "            for n in sample_sizes:\n",
        "                if n in results and key in results[n]:\n",
        "                    s = results[n][key]\n",
        "                    re = s.get(\"re\", np.nan)\n",
        "                    se = s.get(\"re_se\", np.nan)\n",
        "                    out += f\"{(re if np.isfinite(re) else np.nan):5.3f}({(se if np.isfinite(se) else np.nan):.3f})\".rjust(col_w)\n",
        "                else:\n",
        "                    out += f\"{'---':>{col_w}}\"\n",
        "\n",
        "            n0 = sample_sizes[0]\n",
        "            if n0 in results and key in results[n0]:\n",
        "                out += f\"{results[n0][key]['re_asymptotic']:5.3f}\".rjust(last_w)\n",
        "            else:\n",
        "                out += f\"{'---':>{last_w}}\"\n",
        "            print(out)\n",
        "\n",
        "        re_row(\"MLE\", \"MLE\")\n",
        "        for (a1, b1), (a2, b2) in j_pairs:\n",
        "            key = f\"J1({a1},{b1})×J2({a2},{b2})\"\n",
        "            re_row(key, f\"J₁({a1},{b1})×J₂({a2},{b2})\")\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# Runner with ALL J-pairs\n",
        "# ==============================================================\n",
        "\n",
        "def run_etll_simulation_study_designB_robust():\n",
        "    \"\"\"Run Design B simulation with ALL configurations\"\"\"\n",
        "    alpha_true, beta_true, theta_true = 2.0, 0.5, 1.0\n",
        "    sample_sizes = [100,250,500,1000]\n",
        "\n",
        "\n",
        "    j_pairs = [\n",
        "        ((1.0, 1.0), (1.0, 2.0)),\n",
        "        ((1.0, 1.0), (0.3, 1.0)),\n",
        "        ((1.0, 1.0), (1.2, 1.8)),\n",
        "        ((1.0, 1.0), (0.8, 1.0)),\n",
        "        ((1.0, 1.0), (4.0, 12.0)),\n",
        "        ((1.0, 1.0), (2.0, 1.0)),\n",
        "    ]\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ETLL Simulation Study - DESIGN B\")\n",
        "    print(\"Different J (J₁ ≠ J₂), Same h = log(x)\")\n",
        "    print(f\"True params: α={alpha_true}, β={beta_true}, θ={theta_true}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    sim = ETLLSimulation_DesignB(\n",
        "        theta=theta_true,\n",
        "        alpha=alpha_true,\n",
        "        beta=beta_true,\n",
        "        n_quad=250,\n",
        "        use_det_re=True,\n",
        "        use_numeric_info=False,\n",
        "        rng=123\n",
        "    )\n",
        "\n",
        "    results = sim.run_simulation_with_re_se_designB(\n",
        "        sample_sizes=sample_sizes,\n",
        "        j_pairs=j_pairs,\n",
        "        n_batches=50,\n",
        "        sims_per_batch=200,\n",
        "        verbose=True,\n",
        "        ref_at=\"true\"\n",
        "    )\n",
        "\n",
        "    sim.print_results_table_designB(results, sample_sizes, j_pairs)\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "    t0 = time.perf_counter()\n",
        "    results = run_etll_simulation_study_designB_robust()\n",
        "    print(f\"\\n⏱️ Total runtime: {time.perf_counter() - t0:.2f} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTnKGzuio5gK",
        "outputId": "0febda8b-ed09-4a1b-dc8b-603c57ccff3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ETLL Simulation Study - DESIGN B\n",
            "Different J (J₁ ≠ J₂), Same h = log(x)\n",
            "True params: α=2.0, β=0.5, θ=1.0\n",
            "======================================================================\n",
            "\n",
            "Running n=100 with 50 batches...\n",
            "  Batch 1/50\n",
            "  Batch 6/50\n",
            "  Batch 11/50\n",
            "  Batch 16/50\n",
            "  Batch 21/50\n",
            "  Batch 26/50\n",
            "  Batch 31/50\n",
            "  Batch 36/50\n",
            "  Batch 41/50\n",
            "  Batch 46/50\n",
            "\n",
            "Running n=250 with 50 batches...\n",
            "  Batch 1/50\n",
            "  Batch 6/50\n",
            "  Batch 11/50\n",
            "  Batch 16/50\n",
            "  Batch 21/50\n",
            "  Batch 26/50\n",
            "  Batch 31/50\n",
            "  Batch 36/50\n",
            "  Batch 41/50\n",
            "  Batch 46/50\n",
            "\n",
            "Running n=500 with 50 batches...\n",
            "  Batch 1/50\n",
            "  Batch 6/50\n",
            "  Batch 11/50\n",
            "  Batch 16/50\n",
            "  Batch 21/50\n",
            "  Batch 26/50\n",
            "  Batch 31/50\n",
            "  Batch 36/50\n",
            "  Batch 41/50\n",
            "  Batch 46/50\n",
            "\n",
            "Running n=1000 with 50 batches...\n",
            "  Batch 1/50\n",
            "  Batch 6/50\n",
            "  Batch 11/50\n",
            "  Batch 16/50\n",
            "  Batch 21/50\n",
            "  Batch 26/50\n",
            "  Batch 31/50\n",
            "  Batch 36/50\n",
            "  Batch 41/50\n",
            "  Batch 46/50\n",
            "\n",
            "============================================================================================================================================\n",
            "Design B: Standardized MEAN and RE from ETELL(α=2.0, β=0.5, θ=1.0)\n",
            "Different J (J₁ ≠ J₂), Same h = log(x)\n",
            "============================================================================================================================================\n",
            "Weight Config                            n=100                       n=250                       n=500                       n=1000                   n→∞         \n",
            "J₁(a₁,b₁)×J₂(a₂,b₂)                     α̂/α          β̂/β          α̂/α          β̂/β          α̂/α          β̂/β          α̂/α          β̂/β      α̂/α      β̂/β\n",
            "\n",
            "MEAN VALUES:\n",
            "MLE                              1.03(0.014)   0.87(0.149)   1.01(0.009)   0.98(0.096)   1.00(0.006)   0.99(0.066)   1.00(0.004)   0.99(0.047)      1.00      1.00\n",
            "J₁(1.0,1.0)×J₂(1.0,2.0)          1.07(0.014)   0.42(0.148)   1.02(0.009)   0.80(0.095)   1.01(0.006)   0.90(0.066)   1.01(0.004)   0.94(0.047)      1.00      1.00\n",
            "J₁(1.0,1.0)×J₂(0.3,1.0)          1.03(0.016)   0.96(0.180)   1.01(0.010)   1.03(0.114)   1.00(0.007)   1.01(0.077)   1.00(0.005)   1.00(0.054)      1.00      1.00\n",
            "J₁(1.0,1.0)×J₂(1.2,1.8)          1.10(0.015)   0.04(0.147)   1.04(0.009)   0.64(0.095)   1.02(0.006)   0.81(0.066)   1.01(0.005)   0.90(0.047)      1.00      1.00\n",
            "J₁(1.0,1.0)×J₂(0.8,1.0)          1.05(0.015)   0.69(0.157)   1.01(0.010)   0.93(0.102)   1.01(0.007)   0.96(0.070)   1.00(0.005)   0.98(0.049)      1.00      1.00\n",
            "J₁(1.0,1.0)×J₂(4.0,12.0)         1.07(0.017)   0.47(0.177)   1.02(0.010)   0.83(0.108)   1.01(0.007)   0.92(0.074)   1.01(0.005)   0.95(0.053)      1.00      1.00\n",
            "J₁(1.0,1.0)×J₂(2.0,1.0)          1.07(0.014)   0.42(0.148)   1.02(0.009)   0.80(0.095)   1.01(0.006)   0.90(0.066)   1.01(0.004)   0.94(0.047)      1.00      1.00\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "RELATIVE EFFICIENCY:\n",
            "MLE                             0.928(0.010)  0.939(0.010)  0.978(0.011)  0.982(0.010)     1.000\n",
            "J₁(1.0,1.0)×J₂(1.0,2.0)         0.903(0.010)  0.933(0.010)  0.975(0.010)  0.978(0.009)     0.978\n",
            "J₁(1.0,1.0)×J₂(0.3,1.0)         0.753(0.008)  0.782(0.010)  0.839(0.008)  0.854(0.008)     0.852\n",
            "J₁(1.0,1.0)×J₂(1.2,1.8)         0.881(0.011)  0.925(0.010)  0.966(0.010)  0.966(0.010)     0.969\n",
            "J₁(1.0,1.0)×J₂(0.8,1.0)         0.866(0.010)  0.875(0.010)  0.922(0.010)  0.931(0.009)     0.929\n",
            "J₁(1.0,1.0)×J₂(4.0,12.0)        0.752(0.010)  0.816(0.009)  0.870(0.010)  0.875(0.009)     0.887\n",
            "J₁(1.0,1.0)×J₂(2.0,1.0)         0.903(0.010)  0.933(0.010)  0.975(0.010)  0.978(0.009)     0.978\n",
            "\n",
            "⏱️ Total runtime: 13286.49 s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}